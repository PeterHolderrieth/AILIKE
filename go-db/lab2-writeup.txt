Lab2 Writeup
Tally Portnoi

********************************Join Operator********************************
I decided to implement a block hash Join. Since we were given a memory constraint of maxBufferSize, I needed to choose an algorithm where I could limit the number of tuples I store in memory. I did not want to implement any algorithm that would require additional disk storage, so I was deciding between block hash join and block nested loops. I chose block hash join because it is (almost always) faster than block nested loop join: there is some overhead cost associated with building the hashmap for each block, but once you build the hashmap you only need to iterate over the matching hash bucket for each element of the right-hand iterator for each block. With a block nested loop join, you would need to iterate over the entire block for each element of the right-hand iterator for each block, rather than just the matching hash bucket. Both of these algorithms would be faster than simple nested loop join which requires iterating over the entire right-hand iterator for each element of the left-hand iterator.

I constrain memory consumption by only considering at most maxBufferSize tuples from the left-hand iterator in each block. Specifically, I implemented the buildBlockHashMap which inserts at most maxBufferSize tuples from the iterator into a new hashmap on each call. Once I have the block's hashmap, I iterate over the entire right-hand iterator and use the hashmap to find the matches. I repeat this process for each block until the left-hand iterator is empty.

********************************Project Operator********************************
To support select distinct, I use a "seen" map to keep track of unique tuples based on their tupleKey. Tuples with the same values/descriptor would get the same tupleKey value. If distinct=True, I only returned the projected tuple if that tuple's tupleKey was not already present in the "seen" map. If distinct=False, I bypassed the "seen" map and always returned the projected tuple. For each tuple I returned, I also added its tupleKey to the “seen” map. 

********************************Aggregator Operator********************************
Most of the design of the Aggregator was decided for us. We built a map from group keys to AggState slices. That way, for each new tuple, we could create a “group-by tuple” for just the group-by fields using the extractGroupByKeyTuple method, and then call tupleKey() on that tuple to get the group key (this structure was all provided by the course staff). I implemented extractGroupByKeyTuple by evaluating each expression in groupByFields on the tuple and constructing a tuple. Once I had a tuple’s group key, I added that tuple to each AggState for that group's AggState slice; if the AggState slice was new, I initialized each AggState by calling NewAggState[i].Copy() for each position of the AggState slice. Once the child iterator is processed, we call getFinalizedTuplesIterator. For each “group-by tuple” (curGbyTuple), this method gets the group's AggState slice by using its tupleKey and then returns the curGbyTuple joined with its finalized AggState. We return this joined tuples group-by-group.

As a side note, I noticed some weirdness in the Copy() method for MinAggState and MaxAggState: the Copy() method sets the "null" field to true, so it is not a pure copy but more like a copy-and-reset.

********************************Changes to the API********************************
I made one small change to the API, which is to rename the numOpenSlots() method on the Page interface to getNumOpenSlots(). This allowed me to switch from storing numUsedSlots to storing numOpenSlots on heapPage. I changed my implementation to avoid constantly calculating the number of open slots, even though this is not a particularly expensive operation.

********************************Experience Completing the Lab********************************
Once again I had a lot of fun doing this lab. My favorite part was implementing the block hash join. I probably spent 25ish hours doing the lab and write-up. The most time-consuming part of this lab was finding and identifying issues from my lab 1 implementation, which I describe below.

********************************Fixing Issues From Lab 1********************************
While working on TestBigJoinOptional, I noticed the test was timing out before we even got to the join part.  This told me I needed to speed up my implementation of insert tuple. The first thing I tried was to look for pages for inserting from the end of the file rather than from the beginning. This would speed up how long it took me to find an open page, since the end of the file will be more likely to have open pages; however, in the event when the last page is full, I still ended up reading all the pages from disk before deciding to create a new page. To avoid this, I created a "pageFull" map for keeping track of which pages are full. This map defaults to false for all pages, and I set pageFull[i] the first time that page i is read from disk. I update pageFull on reading pages, and inserting/deleting tuples. This saves time since getPageForInsert no longer wastes time reading full pages from disk.

One minor issue from lab 1 took me several hours to debug: I was not calling setDirty(true) on pages after inserting into or deleting from them. None of the tests for lab 1 enforced this, and I thought we could ignore it since we were not dealing with transactions yet. However, the logic for LoadFromCsv depended on the dirty bit getting set in order for pages to get written to disk. This was causing my heap files to be empty until I figured out the issue. I think this could be avoided by adding tests for the dirty bit getting set appropriately in lab 1.

Another bug I found was in my implementation for EvictPage. I had intended to write an eviction policy where I evict the page with the least number of open slots remaining. However, at some point during lab 1 I introduced a bug where I initialized minOpenSlots to 0, and then only updated the page/key to evict when a page had less than or equal minOpenSlots open slots. In practice, this meant I could only evict entirely full pages. I got lucky during lab one since I always had a full page to evict, but I noticed this bug during lab 2 and fixed it. I also improved my implementation for EvictPage by stopping as soon as I find any full page, rather than completing the iteration through all the pages. Fixing the bug by initializing minOpenSlots to math.Inf(1) actually introduced another bug related to go versions which the course staff helped resolve (thank you!).
